\documentclass[11pt]{report}
\usepackage{enumerate}
\usepackage{color}
\usepackage[medium]{titlesec}
\usepackage[paperwidth=8.5in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{mathrsfs}
\usepackage[dvipsnames]{xcolor}
 
%Change Enumeration Environment%
\let\oldenumerate\enumerate
\renewcommand{\enumerate}{
  \oldenumerate
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}

% Code
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{myblue}{RGB}{0,0,204}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\footnotesize,        % size of fonts used for the code
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{myblue},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  keywordstyle=\color{WildStrawberry},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
}


\title{Abstract} 
\author{Qunjie Zhou and Zhenzhang Ye} 
\date{\today}
\begin{document}
\maketitle
\newpage

\section*{Introduction}
\subsection*{Artificial neural network}
In the last couple of years, deep learning techniques have achieved huge success in various signal and information processing tasks including computer vision, automatic speech recognition and natural language processing, producing a great deal of state-of-art. \\
\\
Especially, a variety of deep discriminative models(e.g.,  deep neural networks or DNN, recurrent neural networks or RNN, and convolutional neural networks or CNN, etc) have been applied to fields of vital importance to modern computer science, and have already brought exiting breakthrough.\\ 
\\
Deep learning has such a breathtaking impact, which is not limited to computer science area but to almost every modern technology, that it is valuable to put more investigation and research into it.

\subsection*{Training a neural network}
It has been widely agreed that the deeper these networks are, the better they can learn to tackle complicated real-world applications. However, deeper networks also result in highly non-convex optimization problems, which require very good training algorithms. \\
\\
The traditional back propagation algorithm has the severe problem of being trapped in poor local optima especially when network gets deeper. Recently, stochastic gradient descent (SGD) has been extremely extensively applied for training of deep neural networks. Although SGD performs fairly well in finding a good approximation of global optima, it is difficult to parallelize across machines, which makes learning at large scale nontrivial.  Even though there exists several improved versions(e.g., AdaGrad, Adam, Momentum, etc.), these gradient-based approaches still have the problem of scalability and other issues as well, such as vanishing gradients.\\
\\
The immediate question comes: Is it the best we can do to efficiently learn a neural network?  In fact, viewing training a neural network purely as an optimization problem, gradient descent is surely simple but also naive.  In the family of optimization, a bunch of more sophisticated methods such as ADMM, PDHG are very popular as well. Therefore, we believe it's definitely worth trying others to see whether we can achieve either better accuracy or faster performance.
\\\\
As we focus on applying new optimization methods instead of exploring deep network architectures, we decide to start simple with a not deep multi-layer perceptrons (MLP).

\section*{Approach}
\subsection*{Alternating Direction Method of Multipliers}
One promising option to solve a convex optimization problem  would be using Alternating Direction Method of Multipliers (ADMM). The main idea of ADMM is to decompose the original problem into several smaller sub-problems which are easier to handle and solve. Recently, quite a few researches have applied ADMM to non-convex problems and achieved impressive results. Compared with back propagation, ADMM is more robust to not being trapped in a local optima, since ADMM solves each sub-problem globally. Meanwhile, the decomposition leads to easier parallel programming. Therefore, we decide to apply ADMM to neural network and hope to achieve a dramatic speedups compared with back propagation.

\subsection*{Plan of our project}
To verify if ADMM can work for training neural network, we start our project with MNIST database and a small network. The neural network consists of $L$ layers. There is a linear operator $W_l$, which is also called weight, between adjacent layers and in each layer, each "neuron" is a computational unit that is a active function. Given an input $a_{l-1}$ and its label $y$, the goal of the neural network is to find the best weight $W$ which minimizes a loss function $\mathcal{L}$. To clarify(??), we define the problem as following:
\begin{equation}
\begin{aligned}
& \underset{\{W_l\},\{a_l\}, \{z_l\}}{\text{minimize}}
& & \mathcal{L}(z_L,y) \\
& \text{s.t.}
& & z_l = W_la_{l-1},  l = 1,2,...,L \\
& & & a_l = h_l(z_l),  l = 1,2,...L-1.
\end{aligned}
\end{equation}
Based on [1], we try to analyze this problem and apply ADMM to it. Our first goal is to implement the algorithm from [1] and check its feasibility for this problem. If it works for above problem, we will  analyze this algorithm and  improve the performance. Otherwise, reviewing each part of this algorithm and figure out the correct solution will be included. Afterwards, comparing it with back propagation is one part of our project.\\
After above steps, we can conclude if ADMM is feasible for training neural network. To highlight the advantage of ADMM, applying it to a more complicated and deeper neural network such as CNN or RNN is our further plan.\\
Since the problem in equation (1) is an nonlinear optimization one and this inter discipline project is in Mathematics faculty, we choose the lecture "Nonlinear Optimization: Advanced [MA3503]". The content of this course like optimality conditions will be helpful for analyzing our problem. Some numerical methods introduced in this lecture might give us ideas about solving it.
\end{document}