%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{hyperref}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\usepackage{blindtext}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

\title{\LARGE \bf
Report of Applying ADMM to Neural Network
}


\author{Qunjie Zhou and Zhenzhang Ye% <-this % stops a space
}

\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
\subsection{Artificial Neural Networks} 
Artificial neural networks(ANN) are computational models, inspired by natural neural networks to learn useful information from data. They have been widely applied to tackle real-world problems and brought breakthrough to fields such as image recognition and natural language processing.  Before a neural network can actually perform pattern recognition tasks, it requires numerous learning on a specific dataset, which refers to the training process.
 
 In ANN, a neuron as the core composing unit can be mathematically represented as a function $g: a \rightarrow z$
 \begin{equation}
	g(a, W)= h(\displaystyle\sum_{k}a_kw_k) = h(Wa)
\end{equation}
where $a$ is the input data, $W$ is the weight matrix (Here we consider bias is integrated into $W$) and $h$ is an activation function\cite{1}. A layer is composed by a set of artificial neurons with identical activation function. A feed-forward neural network where the output of neurons of previous layer is feed forward to the neurons of current layer, can be formed by consecutively connecting multiple layers.

To be specific, a 3-layer feed-forward neural network can be mathematical defined as:
\begin{equation} \label{eq:network}
	f(a_0, W) = W_3(h_2(W_2(h_1(W_1a_0))))
\end{equation}
where $W = \{W_1, W_2, W_3\}$ denotes the ensemble of weight ma- trices, and each column of input $a_0$ is a training sample.

\subsection{Training a neural network with SGD }
Such a neural network defined in (\ref{eq:network}) is learned by varying weight matrix $W$ so that the output activation $a_L$ match to the target label $y$. Using a loss function $l$ as the evaluation criteria, the training task can be proposed as an optimization problem:
\begin{equation} \label{eq:min}
	\min_{W} l(f(a_0, W), y)
\end{equation}
As the activation function $h$ introduces non-linearity into the network model, problem (\ref{eq:min}) results in a highly non-convex optimisation problem. The most extensively applied approach for training a neural network is stochastic gradient descent (SGD), using backpropagation to calculate the gradients.  Although SGD performs well in serial setting where modern GPUs are necessary for efficient training, it fails to maintain strong scaling when parallelizing over CPUs machines \cite{2}. In addition, the class of gradient descent based approaches also has the severe problem of being trapped in poor local optima \cite{3} or a saddle point \cite{4}, especially for deeper architecture. 

\subsection{Alternating Direction Method of Multipliers}
In the context of optimisation, except for the gradient-based algorithms, there are still more sophisticated optimisation methods. The alternating direction method with multipliers (ADMM) has been one of most powerful and successful methods for solving constrained convex problems. Recent studies (e.g. \cite{5, 6, 7, 8}) on ADMM for non-convex optimization problem, show that ADMM also performs well on a broad class of solve non-convex problems. Compared with SGD methods, in ADMM the original complex problem is decomposed into several simpler subproblems that are often solvable in closed form. Furthermore, the decomposition leads to possibility of large-scale distributed computing environments.

In this project, we are interested applying the specific optimisation method ADMM to the neural network training task. We follow the approach proposed in work \cite{9} and investigate whether ADMM can get rid of the troubles of SGD mentioned before.  In section \ref{preliminaries}, we state the concrete problem with mathematic notations. The Goldstein ADMM method proposed by work \cite{9} is fully studied in section \ref{studycase}, with practical experiments and theoretical analysis. In the following section \ref{improvement}, an improved version of this method is proposed and demonstrated. Then, the section \ref{experiment}, the improved version is evaluated on different datasets. Finally, section \ref{conclusion} comes to a conclusion based on previous results.

\section{Preliminaries}\label{preliminaries}
To verify the feasibility of training ANN with ADMM, we select the simplest feed-forward neural network as our training model. To define the problem, let's assume the network consists of $L$ layers. Given an input $a_{l-1}$ and its label $y$, the goal of training is to find the best weight $W$ which minimizes a loss function $\mathcal{L}$. The concrete mathematical model is defined as: 
\begin{equation}
\begin{aligned} 
& \underset{\{W_l\},\{a_l\}, \{z_l\}}{\text{minimize}}
& & \mathcal{L}(z_L,y) \\
& \text{s.t.}
& & z_l = W_la_{l-1},  l = 1,2,...,L \\
& & & a_l = h_l(z_l),  l = 1,2,...L-1 \label{eq:model}
\end{aligned}
\end{equation}

Following the idea of \cite{9}, the original unconstrained problem (\ref{eq:min}) is equivalent to the proposed constrained problem (\ref{eq:model}), where separating the objective function at each layer of a neural network into two terms: one term measuring the relation between the weights and the input activations, and the other term containing the nonlinear activation function. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Study on Goldstein ADMM} \label{studycase}
\subsection{The Proposed Method} \label{method}
In work \cite{9}, they proposed a method to solve the constrained problem (\ref{eq:model}) by combining Bregman iterative update with an alternating minimization strategy. They first relax the constraints by adding $l_2$ penalty functions to the objective function, which is actually a quadratic penalty formulation. Then  Lagrange multipliers are applied to output term $z_L$ to exactly enforce equality constraints, which yields the following unconstrained problem: \\
\begin{equation}
\begin{aligned}
\underset{\{W_l\},\{a_l\}, \{z_l\}}{\text{minimize}} & \mathcal{L}(z_L,y) \\
+  \langle z_L, &\lambda \rangle + \beta_L||z_L -W_La_{L-1}||^2 \\
+ \sum_{l=1}^{L-1}&[\gamma_l||a_l - h_l(z_l)||^2 + \beta_l||z_l - W_la_{l_1}||^2]
\label{eq:breg_model}
\end{aligned}
\end{equation}
where $\{\gamma_l \}$ and $\{\beta_l\}$ are constants that control the weight of each constraint; $\lambda$ is a vector of Lagrange multipliers with the same dimensions as $z_L$. 

Noting (\ref{eq:breg_model}) is not a classical ADMM formulation, because the Lagrange multiplier is added only to the objective term $z_L$. A true ADMM formulation should be:
\begin{equation}
\begin{aligned}
\underset{\{W_l\},\{a_l\}, \{z_l\}}{\text{minimize}} & \mathcal{L}(z_L,y) \\
+  \sum_{l=1}^{L}&[\langle \lambda_l, z_l - W_la_{l-1} \rangle + \beta_l||z_l - W_la_{l-1}||^2] \\
+ \sum_{l=1}^{L-1}&[\langle \eta_l, a_l - h_l(z_l) \rangle + \gamma_l||a_l - h_l(z_l)||^2 ]
\label{eq:admm_model}
\end{aligned}
\end{equation}
where $\{\lambda_l\}$, $\{\eta_l\}$ are sets of Lagrange multipliers corresponding to each constraint. They also mentioned the intuition behind (\ref{eq:breg_model}) is to apply Bregmen Iteration \cite{10} for the problem (\ref{eq:model}). 

\subsection{Bregman Iteration Method} \label{breg_method}
Here we will shortly explain the idea of Bregman Iteration Method and derive (\ref{eq:breg_model}) in detail. For a given problem: 
\begin{equation}
\begin{aligned}
min && J(u) && s.t. && Au=b 
\end{aligned}
\end{equation}
where  $J(u)$ is convex and $A$ is a linear operator. The Bregman method iteratively solve:
\begin{equation}
\begin{aligned}
u^{k+1} \leftarrow \underset{u}{\text{argmin}} \: D_J^{p^k}(u, u^k) + {\frac{1}{2}}||Au - b||^2
\label{eq:breg}
\end{aligned} 
\end{equation}
where $p^k \in \partial J(u^k)$ is the subgradient of $J$ at $u^k$ and $D_J^{p^k}(u, u^k) = J(u) - J(u^k) - \langle p^k, u - u^k\rangle $ is called Bregman Distance. Therefore, by definition of $D_J^{p^k}(u, u^k)$, the multiplier is added only to the objective variable $u$, leaving the constraints as $l_2$ penalty terms.

In our problem (\ref{eq:model}), the objective variable is the output layer $z_L$. Thus $J(u) = \mathcal{L}(z_L, y)$ and constraint $Au = b$ corresponds to only $z_L = W_La_{L-1}$. Therefore, applying (\ref{eq:breg}) to (\ref{eq:model}) gives the update of $z_L^{k+1}$:
\begin{equation*}
\begin{aligned}
&z_L^{k+1} = \underset{z_L}{\text{argmin}} \: D_l^{p^k}(z_L, z_L^k) + \beta_L||z_L - W_La_L-1||^2\\
&= \underset{z_L}{\text{argmin}} \: \mathcal{L}(z_L) -\mathcal{L}(z_l^k) 
 - \langle  \partial z_L^k, z_L - z_L^k \rangle +  \beta_L||z_L - W_La_L-1||^2\\
&= \underset{z_L}{\text{argmin}} \: \mathcal{L}(z_L) 
 - \langle \partial(z_L^k), z_L \rangle + \beta_L||z_L - W_La_L-1||^2
 \label{eq:zL}
\end{aligned} 
\end{equation*}
The first-order optimality condition of $z_L$ gives immediately $\lambda$ update:
\begin{equation*}
\begin{aligned}
&0 \in \partial(z_L^{k+1}) -  \partial(z_L^k) + 2 * \beta_L(z_L^{k+1} - W_La_{L-1}) \\
&- \partial(z_L^{k+1}) \in - \partial(z_L^k) +  2 * \beta_L(z_L^{k+1} - W_La_{L-1}) \\
&Setting \: \:  \lambda \in-  \frac{1}{2} \partial(z_L) \\
&\lambda_{k+1} = \lambda^k + \beta_L(z_L^{k+1} - W_La_{L-1})
\label{eq:zL}
\end{aligned} 
\end{equation*}
According to the paper, they somehow combine the Bregmen Iteration with the $l_2$ penalty of the left constraints i.e. constraints except for $z_L$, which leads to exactly (\ref{eq:breg_model}). Then by applying an alternating direction method to formulation (\ref{eq:breg_model}) and addressing each term separately, we get the following algorithm:
\begin{algorithm} 
\caption{ADMM for Neural Networks}
\label{alg:goldADMM}
\KwIn{training features $\{a_0\}$, and labels $\{y\}$}
initialize $\{a_l\}^{L}_{l=1}$, $\{z_l\}^{L}_{l=1}$, $\lambda$ and weights\\
\Repeat{converged}{
	\For{l = 1, 2, \dots, L-1}{
		 $W_l \gets  z_la_{l-1}^\dagger$ \\
		 $a_l \gets (\beta_{l+1}W_{l+1}^TW_{l+1}+\gamma_lI)^{-1}$\\  
	    $~~~~~~ \cdot (\beta_{l+1}W_{l+1}^Tz_{l+1}+\gamma_lh_l(z_l))$\\
	    $z_l \gets argmin_z ||a_l-h_l(z)||^2 + \beta_l||z-W_la_{l-1}||^2$\\
	}
	$W_L \gets z_La_{L-1}^\dagger$\\
	$z_L \gets argmin_z \mathcal{L}(z,y) + <z, \lambda> + \beta_L||z-W_La_{L-1}||^2$\\
	$\lambda \gets \lambda + \beta_L(z_L-W_La_{L-1})$\\
}

\end{algorithm}

\subsection{Implementation and Evaluation}
We implemented the updates of  Algorithm \ref{alg:goldADMM} using Python. In this section, we will explain the details on how we implement these sub-steps. First, we choose the most widely adopted rectified linear units (ReLUs) to be the activation function $h$, which is defined as:
\begin{center}
$h(x) = \begin{cases} x, & \mbox{if } x> 0 \\0, & \mbox{otherwise }\end{cases} $
\end{center}

In the following, we discuss only the updates of $z_l$ and $z_L$, which is not as straightforward as the others.\\
\textbf{Update $z_l$:  }  Calculating the derivative of $h_l(z)$  we get:
\begin{center}
$(\partial_z h_l)_i = \begin{cases} 1, & \mbox{if } x> 0 \\0, & \mbox{otherwise }\end{cases} $
\end{center}
Then we get element-wise update of $(z_l)_i$ in following closed form:
\begin{center}
$(z_l)_i = \begin{cases} \frac{\gamma_la_l + \beta_lW_la_{l-1}}{\gamma_l + \beta_l}, & \mbox{if} (z_l)_i> 0 \\(W_la_{l-1})_i, & \mbox{otherwise}\end{cases} $
\end{center}
We do projection of $(z_l)_i$ to corresponding feasible space and pick the $(z_l)_i$ leading to the smaller value of $(l_{quad})_i$ where $l_{quad} = \gamma_l||a_l - h_l(z_l)||^2 + \beta_l||z_l - W_la_{l_1}||^2$ contains the $l_2$ penalty terms. \\\\
\textbf{Update $z_L$:  }  We have to first decide the choice of loss function $l$. We tried to use hinge loss, mean square and softmax cross entropy, it turned out softmax has the best numerical result. However, using softmax as the loss function makes the $z_L$ update unsolvable in a closed form. Thus, we tried to use both gradient descent and proximal gradient to solve the minimization subproblem i.e. the update of $z_L$. Both of them worked out and achieved reasonable results. 

To evaluate the method, we tested it on Mnist dataset with different network architectures i.e. various number of hidden layers and its dimension. The weight matrix $W$ is initialized with random samples from Gaussian distribution with zero mean and standard deviation $\epsilon^2$. 

According to the methods published in "The MNIST Dataset" \footnote{http://yann.lecun.com/exdb/mnist/}, ANN should achieve at least 95\% accuracy.  Also, the tutorial implementation from Tensorflow also results an accuracy around 92\%. However, after we tuned a list of hyper-parameters (e.g. $\epsilon$, $\beta_l$, $\gamma_l$, etc.),  the best accuracy our network can achieve is around 87\%, which is far away from the result stated on the Mnist website. 

\subsection{Analysis}
In this section, we want to investigate the potential reasons of the low performance of our model. By looking at the energy, we found that the energy does not converge smoothly to the optimal. As illustrated by the Fig. \ref{fig:energySame}, the energy stops changing around 2.0 and it is supposed to decay to 0.

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figure/energy_has_lambda.png}
\caption{The energy converges to 2.08 with Goldstein ADMM}
\label{fig:energySame}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\columnwidth]{figure/energy_no_lambda.png}
\caption{The energy converges to 0 without $\lambda$}
\label{fig:energyWithoutLambda}
\end{figure}

Recall the difference between the Goldstein ADMM and the classical ADMM, the Lagrange multiplier and the Bregman iteration might contribute to the low performance. Therefore, we remove the Lagrange term and only include the quadratic penalties into objective function.

We apply Algorithm \ref{alg:goldADMM} with $\lambda=0$ and deleting line 10 to solve equation \eqref{eq:quadratic_only}. A result with lower energy and higher accuracy is achieved, which is illustrated in Fig.\ref{fig:energyWithoutLambda}. Based on this fact, we guess the Bregman iteration and the Lagrange multiplier are the main reason for the low performance.

To analyze the reason mathematically, we introduce a simple problem:
\begin{equation}
\begin{aligned}
& \underset{w,a}{\text{minimize}}  & & (a-1)^2 \\
& \text{s.t.} & & w+a =2\\
\end{aligned}
\label{eq:demo}
\end{equation}
which can be viewed as an ANN with 1 layers and the dimension of $a$ is $\mathbb{R}$. Since the Bregman iteration is applied for linear problems, we simplify $w \cdot a$ with $w + a$. Applying the idea of equation \eqref{eq:breg_model}:
\begin{equation}
\begin{aligned}
& \underset{w,a}{\text{minimize}}  & & (a-1)^2 + \lambda \cdot a + \beta (2-w-a)^2\\
\end{aligned}
\end{equation}
Above problem can be solve by using Algorithm \ref{alg:goldADMM}, which giving us:
\begin{equation}
\begin{aligned}
w^{k+1} &= 2-a^k \\
a^{k+1} &= \frac{\lambda^k+2+4\beta-2\beta w^{k+1}}{2+2\beta}\\
\lambda^{k+1} &= \lambda^{k+1} + 2\beta(2-w^{k+1}-a^{k+1})\\
\end{aligned}
\label{eq:demoupdate}
\end{equation}
This test example cannot return the optimal solution $w=1,a=1$ unless one of them is initialized as 1. However, the result can always satisfy the constraint. This is because of the property of Bregman iteration: once a feasible result is achieved, the iteration stops and it is the optimal one. It is obvious that when $w^k$ and $a^k$ satisfy the constraints, equation \eqref{eq:demoupdate} will stop updating. To show the rest of this property, recall that Bregman iteration wants to minimize the Bregman distance $D^p_J(u,v) = J(u)-J(v)-<p,u-v>, p \in \partial J(v)$ which is nonnegative. Therefore, substitute our problem into the nonnegative inequality, for any $a$, we can get
\begin{equation}
\begin{aligned}
J(a^k) & \leq J(a) -(a-a^k) \cdot \lambda^k \\
& = J(a) - (a-(2-w^k)) \cdot \lambda^k \\
\end{aligned}
\end{equation}
which means for any $a$ that satisfies $a+w^k=2$, we can get the optimal $J(a)$. However, we cannot ensure that this $w^k$ is the optimal value, i.e. if the final $w^k$ is not optimal, the corresponding $a$ is not optimal.

In fact, we can construct a new value $u = [w,a]^T$ and rewrite the problem in equation \eqref{eq:demo}
\begin{equation}
\begin{aligned}
& \underset{u}{\text{minimize}}  & & ([0,1]\cdot u-1)^2 \\
& \text{s.t.} & & [1,1] \cdot u = 2\\
\end{aligned}
\end{equation}
then if we apply Bregman iteration to above problem, which is classical Bregman iteration, the result is correct. In this case, the Lagrange multiplier $\lambda$ is a 2x1 matrix, which means updating $w$ should consider $\lambda$.

All above are discussed in linear case, we also tested the same idea in nonlinear case, this Bregman iteration cannot work neither. Therefore, we conclude that the reason why Goldstein ADMM cannot work is that the Bregman iteration and Lagrange multiplier should not only be considered for $z_L$ only.

\section{Improvement based on Goldstein ADMM} \label{improvement}
In this section, we propose our improvement based on Goldstein ADMM. We first try to solve equation \eqref{eq:model} using classical ADMM. It turns out that classical ADMM cannot perform stably. Inspired by the quadratic penalty method, we include only $l_2$ norm penalty in to objective function. This method can return a sequence with decaying energy to 0. Nonetheless, the constraints in equation \eqref{eq:model} are satisfied slowly and even not satisfied if the parameter $\gamma, \beta$ are not chose correctly. Additionally, we notice that if $\beta$ in Algorithm \ref{alg:goldADMM} is sufficient small, it is almost same as the quadratic penalty method. Therefore, we figure out one algorithm which can converge to optimal solution to a certain degree and the constraints are satisfied.

\subsection{Quadratic penalty method}
According to previous analysis, the Lagrange multiplier and Bregman iteration are the causes why Algorithm \ref{alg:goldADMM} fails. Thus, we manage to solve the equation \eqref{eq:model} by using quadratic penalty method, which leads to the equation:
\begin{equation}
\begin{aligned}
\underset{\{W_l\},\{a_l\}, \{z_l\}}{\text{minimize}} 
& l(z_L,y) + \beta_L||z_L -W_La_{L-1}||^2 \\
+ \sum_{l=1}^{L-1}&[\gamma_l||a_l - h_l(z_l)||^2 + \beta_l||z_l - W_la_{l_1}||^2]
\label{eq:quadratic_only}
\end{aligned}
\end{equation}
This optimization problem can be solved by Algorithm \ref{alg:goldADMM} with a little modification mentioned before.

\begin{figure*}
\centering
\begin{subfigure}{0.8\columnwidth}
\centering
\includegraphics[width=0.7\columnwidth]{figure/aConstraint_has_lambda.png}
\caption{$||a_l-h_l(z_l)||^2$ with Lagrange multiplier $\lambda$}
\end{subfigure}
\begin{subfigure}{0.8\columnwidth}
\centering
\includegraphics[width=0.7\columnwidth]{figure/zConstraint_has_lambda.png}
\caption{$||z_l-W_la_{l-1}||^2$ with Lagrange multiplier $\lambda$}
\end{subfigure}
\begin{subfigure}{0.8\columnwidth}
\centering
\includegraphics[width=0.7\columnwidth]{figure/aConstraint_no_lambda.png}
\caption{$||a_l-h_l(z_l)||^2$ without Lagrange multiplier $\lambda$}
\end{subfigure}
\begin{subfigure}{0.8\columnwidth}
\centering
\includegraphics[width=0.7\columnwidth]{figure/zConstraint_no_lambda.png}
\caption{$||z_l-W_la_{l-1}||^2$ without Lagrange multiplier $\lambda$}
\end{subfigure}
\caption{The norm of constraints, $l=1$}
\label{fig:cmpCons}
\end{figure*}

It is obvious that once the optimal value is got, the corresponding $\{W_l\}$, $\{a_l\}$,$\{z_l\}$ are feasible for equation \eqref{eq:model}. However, the experiments show that the norms of constraints decays slowly, as illustrated in Fig.\ref{fig:cmpCons}. This is caused by the property of ADMM that original complex problem is decomposed into several simpler subproblems. Updating $\{a_l\}$ and $\{z_l\}$ become a trade-off between satisfying the constraints and minimizing loss function. If increasing the weight of each constraint, constraints are satisfied faster with higher loss energy. Additionally, increasing the iteration number can satisfy the constraints and lower the loss energy at the same time, which accounts for more computational expensive especially for large dataset.

\subsection{Bregman iteration with increasing penalty weights}
To overcome the disadvantage of quadratic penalty method, a time-saving algorithm, which can satisfy constraints and minimize loss function at the same time, is proposed. Inspired by above discussion about the weight of each constraints, in this algorithm, the penalty weights are increasing at each iteration. To improve the efficiency, we also implement mini-batch for this algorithm. Besides, the regularization item is involved in the object function to prevent over-fitting and numerical errors.

\begin{equation}
\begin{aligned} 
& \underset{\{W_l\},\{a_l\}, \{z_l\}}{\text{minimize}}
& & l(z_L,y) + \sum_{l=1}^L \alpha ||W_l||^2 \\
& \text{s.t.}
& & z_l = W_la_{l-1},  l = 1,2,...,L \\
& & & a_l = h_l(z_l),  l = 1,2,...L-1 \label{eq:modelWithReg}
\end{aligned}
\end{equation}
where $\alpha$ is the weight of regularization. The new algorithm to solve this optimization problem is described in Algorithm \ref{alg:ourADMM}.
\begin{algorithm} 
\caption{ADMM for Neural Networks}
\label{alg:ourADMM}
\KwIn{training features $\{a_0\}$, and labels $\{y\}$}
initialize $\{a_l\}^{L}_{l=1}$, $\{z_l\}^{L}_{l=1}$, $\lambda$ and weights\\
\For{i = 1, \dots, outIteration}{
	$a_0 \gets$ random batch\\
	\For {j = 1, \dots, inIteration}{
		\For{l = 1, 2, \dots, L-1}{
		 	$W_l \gets (\alpha+I)^{-1}z_la_{l-1}^T(I+a_{l-1}a_{l-1}^T)^{-1}$ \\
		 	$a_l \gets (\beta_{l+1}W_{l+1}^TW_{l+1}+\gamma_lI)^{-1}$\\  
	    		$~~~~~~ \cdot (\beta_{l+1}W_{l+1}^Tz_{l+1}+\gamma_lh_l(z_l))$\\
	    		$z_l \gets argmin_z ||a_l-h_l(z)||^2 + \beta_l||z-W_la_{l-1}||^2$\\
	    		$\beta_l \gets \beta_l \cdot \rho $
		}
		$W_L \gets (\alpha+I)^{-1}z_La_{L-1}^T(I+a_{L-1}a_{L-1}^T)^{-1}$\\
		$z_L \gets argmin_z \mathcal{L}(z,y)$\\
		$ ~~~~~~+ <z, \lambda> + \beta_L||z-W_La_{L-1}||^2$\\
		$\lambda \gets \lambda + \beta_L(z_L-W_La_{L-1})$\\
		$\beta_L \gets \beta_L \cdot \rho$\\
	}
}

\end{algorithm}

\textbf{Increasing penalty weights}: according to the discussion in quadratic penalty method, the weights of penalty play an important role in satisfying constraints and minimizing loss function. Therefore, the main idea is that the initial value of the weights are sufficient small and at each iteration, the weights are multiplied by a constant $\rho>1$. We also involve the Lagrange multiplier $\lambda$ and apply the same Bregman iteration as Goldstein ADMM to force the constraints to be satisfied faster. The main reason is that with small $\{\beta_L\}$, the $\lambda$ has ignorable effect on updating $z_L$ since $\lambda$ is initialized with 0. Thus, at the beginning iterations, a $z_L$ which can minimizing the loss function sufficiently, is achieved. When the $\{\beta_L\}$ becomes large enough, $z_L$ has to satisfy the constraints as well. To validate our idea, we still use equation \eqref{eq:demo}. $\beta$ is initialized with 0.0001 and $\rho$ is 1.01. For each iteration $k$, we get
\begin{equation}
\begin{aligned}
w^{k+1} &= 2-a^k \\
a^{k+1} &= \frac{\lambda^k+2+4\beta-2\beta w^{k+1}}{2+2\beta}\\
\lambda^{k+1} &= \lambda^{k+1} + 2\beta(2-w^{k+1}-a^{k+1})\\
\beta &= \beta \cdot \rho \\
\end{aligned}
\label{eq:demoupdate}
\end{equation}
The test result is $x=1.0001, w=0.9999$. Even if the iteration number is increased, it cannot converge to the optimal result, which means this algorithm can only converges to the neighborhood of the optimal result. The radius of the neighborhood depends on the initial value of $\beta$. If $\beta$ is initialized with smaller positive value, a final result which is closer to optimal one is achieved, which coincides with above analysis. Another parameter $\rho$ is determined based on the initial value of $\beta$. Small $\rho$ leads to a slow speed of satisfying the constraints. Too large $\rho$ results in a high loss energy. Besides, this algorithm is also tested on nonlinear case and can work as well. Therefore, even with the disadvantage that it can only reach the neighborhood, we decide to apply this algorithm to solve equation \eqref{eq:model}.



\textbf{Other improvement}: We implement the mini-batch to our algorithm. The basic idea is that a batch is randomly chose from the dataset. Then the neural network is trained by this batch. After several iterations, a new batch is chose and train the neural network based on previous results. Besides, we also include the regularization item for $\{W_l\}$ to solve the numerical errors and over-fitting. The new problem then becomes:


\section{Experiments} \label{experiment}
In this section, we show the numeric results of Alg \ref{alg:ourADMM} on three different datasets: MNIST and CresentMoon. 

\subsection{MNIST}
For this dataset, two neural network are applied. The first neural network contains one hidden layer of 300 neurons. Another neural network contains two hidden layers. The first hidden layer contains 500 neurons while the second one contains 300 neurons. To train the neural network, we apply mini batch to both of them. The batch size is 400. The total training set are 60,000 examples and no validation examples. Other parameters and the final result are showed in table \ref{tab:parMNIST}.



The accuracy is not as good as the same neural network published on the website. Therefore, we also try to train the neural network without mini batch. The results become better than this which can achieve a accuracy around 97\% and energy smaller than 0.1. 

\begin{table*}[h]
\centering
\caption{Results on Mnist}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
	Hidden Layers & $\{\beta_l\}$ & $\{\gamma_l\}$ & $\rho$ & inIteration & outIteration & weight decay & final energy & train accuracy & test accuracy\\
\hline
		(300) & 1e-6 & 1e-6 & 1.05 & 4 & 400 & 1e-3 & 0.15378 & 95.4\% & 94.8\% \\
		(500,300) & 1e-8 & 1e-8 & 1.05 & 4 & 400 & 1e-3 & 0.15871 & 95.2 \% & 95.1\% \\ 
\hline
\end{tabular}
\label{tab:parMNIST}
\end{table*}

\begin{table*}[h]
\centering
\caption{Results on CrescentMoon}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
	Hidden Layers & $\{\beta_l\}$ & $\{\gamma_l\}$ & $\rho$ & inIteration & outIteration & weight decay & final energy & train accuracy & test accuracy\\
\hline
		(300) & 1e-6 & 1e-6 & 1.05 & 4 & 400 & 1e-3 & 0.15378 & 95.4\% & 94.8\% \\
		(500,300) & 1e-8 & 1e-8 & 1.05 & 4 & 400 & 1e-3 & 0.15871 & 95.2 \% & 95.1\% \\ 
\hline
\end{tabular}
\label{tab:parMoon}
\end{table*}

\subsection{CrescentMoon}
For this dataset, two neural network are applied. The first neural network contains one hidden layer of 300 neurons. Another neural network contains two hidden layers. The first hidden layer contains 500 neurons while the second one contains 300 neurons. To train the neural network, we apply mini batch to both of them. The batch size is 400. The total training set are 60,000 examples and no validation examples. Other parameters and the final result are showed in table \ref{tab:parMoon}.



The accuracy is not as good as the same neural network published on the website. Therefore, we also try to train the neural network without mini batch. The results become better than this which can achieve a accuracy around 97\% and energy smaller than 0.1. 




\section{Conclusion} \label{conclusion}

%\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{99}

\bibitem{1} "The Machine Learning Dictionary" , \url{http://www.cse.unsw.edu.au/~billw/mldict.html}

\bibitem{2} Dean, Jeffrey, et al. "Large scale distributed deep networks." Advances in neural information processing systems. 2012.


\bibitem{3} Choromanska, Anna, et al. "The Loss Surfaces of Multilayer Networks." AISTATS. 2015.

\bibitem{4} Dauphin, Yann N., et al. "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization." Advances in neural information processing systems. 2014.

\bibitem{5} Xu, Zheng, et al. "An empirical study of admm for nonconvex problems." arXiv preprint arXiv:1612.03349 (2016).

\bibitem{6} Hong, Mingyi, Zhi-Quan Luo, and Meisam Razaviyayn. "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems." SIAM Journal on Optimization 26.1 (2016): 337-364.

\bibitem{7} Wang, Yu, Wotao Yin, and Jinshan Zeng. "Global convergence of ADMM in nonconvex nonsmooth optimization." arXiv preprint arXiv:1511.06324 (2015).

\bibitem{8} Wang, Fenghui, Zongben Xu, and Hong-Kun Xu. "Convergence of Bregman alternating direction method with multipliers for nonconvex composite problems." arXiv preprint arXiv:1410.8625 (2014).

\bibitem{9} Taylor, Gavin, et al. "Training neural networks without gradients: A scalable admm approach." International Conference on Machine Learning. 2016.

\bibitem{10} Goldstein, Tom, and Stanley Osher. "The split Bregman method for L1-regularized problems." SIAM journal on imaging sciences 2.2 (2009): 323-343.

\end{thebibliography}
\end{document}
