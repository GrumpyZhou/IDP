\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Short sum up}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Theoretical aspect}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Implementation aspect}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Accuracy comparison between different loss functions with Mnist dataset}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Tuning Hyperparameters in the case of using softmax and proximal gradient }{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Comparison of different $\epsilon $}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Comparison of different network structure}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Comparison of iteration times}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Comparison of step size }{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Comparison of updating $\beta $ and $\gamma $}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusion}{4}}
