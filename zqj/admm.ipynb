{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADMM applied in Neural Networks with Mnist dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Preparation with environment for notebook\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from NeuralNetwork.data_utils import *\n",
    "\n",
    "# Allow inline matplotlib figures appear in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Allow auto-reload of external modules \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Loading of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (60000, 28, 28)\n",
      "Training labels shape:  (60000,)\n",
      "Test data shape:  (10000, 28, 28)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load Mnist Data\n",
    "mnistDir = \"NeuralNetwork/MnistData\"\n",
    "X_train,Y_train,X_test,Y_test = getMnistData(mnistDir)\n",
    "\n",
    "# Check the size of the training and test data.\n",
    "print 'Training data shape: ', X_train.shape\n",
    "print 'Training labels shape: ', Y_train.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAABaCAYAAADkQh/tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGcbYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cf36410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display part of the Mnist data\n",
    "for xi in range(0,9):\n",
    "    plt.subplot(1,10,xi+1)\n",
    "    plt.imshow(X_train[xi])\n",
    "    plt.axis('off')\n",
    "    plt.title(Y_train[xi])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from NeuralNetwork.admm import *\n",
    "import time\n",
    "def getStatistics(Xtr, Ytr, Xte, Yte, weightConsWeight, activConsWeight, iterNum, epsilon):\n",
    "    # Initialize a 3-layer neural network with specified neuron dimension, \n",
    "    # the first dim is determined by the size of input dataset\n",
    "    neuronDim = [0, 150, 150]\n",
    "    network = NeuralNetwork(3, neuronDim)\n",
    "    classNum = 10\n",
    "    \n",
    "    # Initialize weight matrix W of first layer \n",
    "    W = np.random.randn(Xtr.shape[1], classNum) * epsilon\n",
    "\n",
    "    # Train\n",
    "    tic = time.time()\n",
    "    weight = network.train(W, Xtr, Ytr, weightConsWeight, activConsWeight, iterNum, epsilon)\n",
    "    toc = time.time()\n",
    "    print 'Total training time: %fs' % (toc - tic)\n",
    "    \n",
    "    # Predict with the trained feature weight\n",
    "    Ypred = network.predict(weight, Xte)\n",
    "    print 'Prediction accuracy: %f' %np.mean(Ypred == Yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a neural network:  3  layers; Neuron Dimension: [0, 1024, 1024]\n",
      "Total training time: 27.253368s\n",
      "Prediction accuracy: 0.160000\n"
     ]
    }
   ],
   "source": [
    "def miniPatchTest(trNum, teNum):\n",
    "    # Subsample the data for more efficient code execution \n",
    "    trainNum = trNum\n",
    "    mask = range(trainNum)\n",
    "    Xtr = X_train[mask]\n",
    "    Ytr = Y_train[mask]\n",
    "\n",
    "    testNum = teNum\n",
    "    mask = range(testNum)\n",
    "    Xte = X_test[mask]\n",
    "    Yte = Y_test[mask]\n",
    "    \n",
    "    # Reshape the image data into rows\n",
    "    Xtr = np.reshape(Xtr, (Xtr.shape[0], -1))\n",
    "    Xte = np.reshape(Xte, (Xte.shape[0], -1))\n",
    "#     print Xtr.shape, Xte.shape\n",
    "\n",
    "    # Specify weight coefficients of two regularization term, iteration of ADMM updates\n",
    "    weightConsWeight = 1\n",
    "    activConsWeight = 10\n",
    "    iterNum = 10\n",
    "    epsilon = 0.01\n",
    "    getStatistics(Xtr, Ytr, Xte, Yte, weightConsWeight, activConsWeight, iterNum, epsilon)\n",
    "    \n",
    "\n",
    "train = np.array([1000])\n",
    "for i in train:\n",
    "    miniPatchTest(i,100)\n",
    "# print \"==================\"\n",
    "# for i in train:\n",
    "#     miniPatchTest(i,5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " C:10 N:200 L:3 epsilon:0.000100    [784, 100, 150]\n"
     ]
    }
   ],
   "source": [
    "# Testing Module\n",
    "C = W.shape[1]\n",
    "N = Xtr.shape[0]\n",
    "L = 3\n",
    "Dim = neuronDim\n",
    "Dim[0] = Xtr.shape[1]\n",
    "epsilon = 0.0001\n",
    "\n",
    "\n",
    "print \"C:%d N:%d L:%d epsilon:%f \" %(C,N,L,epsilon), \" \", Dim,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main\n",
    "def neuronReLU(x):\n",
    "        \"\"\" Evaluate Rectified Linear Unit \"\"\"\n",
    "        # element wise\n",
    "        xn = np.copy(x)\n",
    "        xn[xn < 0] = 0\n",
    "        return xn\n",
    "    \n",
    "def toHotOne(Y, C):\n",
    "        \"\"\" Construct Hot-one representation of Y \"\"\"\n",
    "        y = np.zeros((Y.shape[0],C))\n",
    "        for i in range(0,Y.shape[0]):\n",
    "            y[i, Y[i]] = 1\n",
    "        return y\n",
    "\n",
    "def outputElementWiseCost(beta, aw, z, y, isOne):\n",
    "        return hingeLossElementWiseCost(z,y, isOne) + beta * (z - aw) ** 2 \n",
    "\n",
    "def regularElementWiseCost(beta, gamma, a, aw, z):\n",
    "        \"\"\" Calculate elementwise cost \"\"\"\n",
    "        return  gamma * (a - neuronReLU(z)) ** 2 + beta * (z - wa) ** 2 \n",
    "    \n",
    "def hingeLossElementWiseCost(z, y, isOne):\n",
    "        \"\"\" Evaluate Hinge Loss \"\"\"\n",
    "        zn = np.copy(z)\n",
    "        x = np.zeros(z.shape)\n",
    "        if not isOne:\n",
    "            zn[y == 0] = np.maximum(zn,x)[y == 0]\n",
    "        else:\n",
    "            zn[y == 1] = np.maximum(1-zn,x)[y == 1]\n",
    "        return zn\n",
    "\n",
    "def regularElementWiseCost(beta, gamma, a, aw, z):\n",
    "        \"\"\" Calculate elementwise cost \"\"\"\n",
    "        return  gamma * (a - neuronReLU(z)) ** 2 + beta * (z - aw) ** 2 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " a:  (200, 784) (200, 100) (200, 150) \n",
      " z:  (0,) (200, 100) (200, 150) (200, 10) \n",
      " w:  (0,) (784, 100) (100, 150) (150, 10)\n",
      "w 1 :  [[  6.73174109e-21  -3.55177491e-22  -3.00971339e-21  -6.47328761e-22]\n",
      " [ -6.69472848e-21  -9.62135974e-22  -2.23472719e-21  -5.81547743e-22]\n",
      " [ -6.93336007e-21   1.60449773e-22   1.23836964e-22   7.52271444e-23]\n",
      " [  1.40218760e-21  -1.00658415e-21   1.97234936e-21  -2.11409582e-21]\n",
      " [  1.37897920e-20   1.08097370e-21   2.93737749e-21  -2.12517557e-22]]\n",
      "w 2 :  [[-0.05120213 -0.10317951 -0.01531815  0.12185371]\n",
      " [ 0.17040168 -0.05947507  0.46553075 -0.01965603]\n",
      " [ 0.28273424  0.32104814 -0.20826831 -0.45522063]\n",
      " [ 0.20066342 -0.10783917 -0.46707249 -0.27588645]\n",
      " [-0.15215018 -0.45553279 -0.28327806  0.40028184]]\n",
      "w 3 :  [[-0.006073   -0.06824106  0.27906912  0.32243342]\n",
      " [-0.03666355  0.02531272  0.40318304  0.28581778]\n",
      " [ 0.12596102 -0.45680816  0.39253417 -0.36061932]\n",
      " [ 0.45670397  0.0740398  -0.31883678 -0.59107709]\n",
      " [-0.37295304  0.78690997  0.3894381  -0.34976201]]\n"
     ]
    }
   ],
   "source": [
    "# Initialization \n",
    "# - a[]: activation list for each layer [a0, a1, a2]: a0(N,Dim[0]), a1(N,Dim[1]),a2(N,Dim[2])\n",
    "# - z[]: z list for each layer [0, z1, z2, z3]: z1(N,Dim[1]),z2(N,Dim[2]), z3(N,C)\n",
    "# - w[]: weight list for each layer  [0, w1, w2, w3]: w1(Dim[0],Dim[1]), w2(Dim[1],Dim[2]), w3(Dim[2],C)\n",
    "\n",
    "a = [Xtr, epsilon*np.random.randn(N, Dim[1]), epsilon*np.random.randn(N, Dim[2])] # whether to vectorize\n",
    "z = [np.zeros((0)), epsilon*np.random.randn(N, Dim[1]), epsilon*np.random.randn(N, Dim[2]), epsilon*np.random.randn(N, C)]\n",
    "w = [np.zeros((0)), epsilon*np.random.randn(Dim[0], Dim[1]), epsilon*np.random.randn(Dim[1], Dim[2]), epsilon*np.random.randn(Dim[2], C)]\n",
    "\n",
    "# - beta,gama: penalty coefficiencies\n",
    "# - K: it  erations of ADMM\n",
    "beta = 1.0 * weightConsWeight # 1\n",
    "gamma = 1.0 * activConsWeight\n",
    "K = iterNum\n",
    "\n",
    "print \"\\n a: \",\n",
    "for ai in a:\n",
    "    print ai.shape,\n",
    "\n",
    "print \"\\n z: \",\n",
    "for ai in z:\n",
    "    print ai.shape,\n",
    "\n",
    "print \"\\n w: \",\n",
    "for ai in w:\n",
    "    print ai.shape,\n",
    "\n",
    "print \n",
    "\n",
    "# Main part of ADMM updates\n",
    "for k in range(K):\n",
    "    # Walk through L-layer network\n",
    "    for l in range(1, L):\n",
    "        w[l] = np.linalg.pinv(a[l-1]).dot(z[l])\n",
    "        wNtr = w[l+1].T\n",
    "#         print  z[l+1].shape,wNtr.shape,w[l+1].shape\n",
    "        x = beta * w[l+1].dot(wNtr) + gamma * np.identity(wNtr.shape[1])\n",
    "#         print \"\",x[:5,:4], np.linalg.det(x),\n",
    "        \n",
    "        #a[l] = np.linalg.inv(beta * (w[l+1].dot(wNtr) + gamma)) * (beta * z[l+1].dot(wNtr)) + gamma * neuronReLU(z[l])\n",
    "        a[l] = (beta * z[l+1].dot(wNtr) + gamma * neuronReLU(z[l])).dot(np.linalg.inv(beta * w[l+1].dot(wNtr) + gamma * np.identity(wNtr.shape[1])))\n",
    "\n",
    "        \n",
    "        \n",
    "        # z update\n",
    "        aw = a[l-1].dot(w[l])\n",
    "        \n",
    "        # z_i < 0\n",
    "        z_s = np.copy(aw)\n",
    "        z_s[z_s > 0] = 0\n",
    "        l_s = regularElementWiseCost(beta, gamma, a[l], aw, z_s)\n",
    "\n",
    "        # z_i > 0\n",
    "        z_b = (gamma * a[l] + beta * z_s) / (beta + gamma)\n",
    "        z_b[z_b < 0] = 0\n",
    "        l_b = regularElementWiseCost(beta, gamma, a[l], aw, z_b)\n",
    "        \n",
    "        z_s[l_s > l_b] = z_b[l_s > l_b]\n",
    "        z[l] = np.copy(z_s)\n",
    "      \n",
    "      \n",
    "    # L-layer\n",
    "    w[L] = np.linalg.pinv(a[L-1]).dot(z[L])\n",
    "    \n",
    "\n",
    "    # Transform y to hotone representation\n",
    "    y = toHotOne(Ytr, C)\n",
    "\n",
    "    # Hinge Loss\n",
    "    awL = a[L-1].dot(w[L])\n",
    "    zL = np.zeros(awL.shape)\n",
    "\n",
    "    # y_i = 1\n",
    "    # zi > 1\n",
    "    zL_b = np.copy(awL)\n",
    "    zL_b[zL_b < 1] = 1\n",
    "    lL_b = outputElementWiseCost(beta, awL, zL_b, y, 1)\n",
    "    \n",
    "    # zi < 1\n",
    "    zL_s = np.copy(awL + 1 / (2 * beta))\n",
    "    zL_s[zL_s > 1] = 1\n",
    "    lL_s = outputElementWiseCost(beta, awL, zL_s, y, 1)\n",
    "    \n",
    "    zL_s[lL_s > lL_b] = zL_b[lL_s > lL_b]\n",
    "    zL[y == 1] = zL_s[y == 1]\n",
    "\n",
    "    # y_i = 0\n",
    "    # zi < 0\n",
    "    zL_s = np.copy(awL)\n",
    "    zL_s[zL_s > 0] = 0\n",
    "    lL_s = outputElementWiseCost(beta, awL, zL_s, y, 0)\n",
    "\n",
    "    # zi > 0\n",
    "    zL_b = np.copy(awL - 1 / (2 * beta))\n",
    "    zL_b[zL_b < 0] = 0\n",
    "    lL_b = outputElementWiseCost(beta, awL, zL_b, y, 0)\n",
    "        \n",
    "    zL_s[lL_s > lL_b] = zL_b[lL_s > lL_b]\n",
    "    zL[y == 0] = zL_s[y == 0]\n",
    "    \n",
    "    # Update zL\n",
    "    z[L] = zL\n",
    "    \n",
    "    # Update beta, gamma\n",
    "    beta *= 1.05\n",
    "    gamma *= 1.05\n",
    "    \n",
    "    # Calculate loss\n",
    "    #loss = calcuLoss(w, a, z, Ytr, gamma, beta)\n",
    "    #print \"Loss of iter \",k,\":\", loss\n",
    "\n",
    "for i in range(1,L+1):\n",
    "    print \"w\",i,\": \", w[i][:5,:4]\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
