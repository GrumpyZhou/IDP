{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist with TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Allow auto-reload of external modules \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-connected Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Eval:\n",
      "100 10000\n",
      "0.9033\n",
      "  Num examples: 10000  Num correct: 9033  Precision @ 1: 0.9033\n",
      "Step 1999: loss = 0.41\n"
     ]
    }
   ],
   "source": [
    "import NeuralNetwork.TensorFlow.simple_nn as nn\n",
    "hidden = 300\n",
    "w1,w2,loss, logit = nn.getTrainedWeight(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.27 (0.022 sec)\n",
      "Step 200: loss = 1.36 (0.005 sec)\n",
      "Step 400: loss = 1.00 (0.005 sec)\n",
      "SHUFFLE\n",
      "Step 600: loss = 0.70 (0.005 sec)\n",
      "Step 800: loss = 0.47 (0.005 sec)\n",
      "Step 1000: loss = 0.47 (0.005 sec)\n",
      "SHUFFLE\n",
      "Step 1200: loss = 0.41 (0.005 sec)\n",
      "Step 1400: loss = 0.56 (0.005 sec)\n",
      "Step 1600: loss = 0.33 (0.005 sec)\n",
      "SHUFFLE\n",
      "Step 1800: loss = 0.45 (0.005 sec)\n",
      "Final Eval:\n",
      "100 10000\n",
      "  Num examples: 10000  Num correct: 8996  Precision @ 1: 0.8996\n",
      "Step 1999: loss = 0.38\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os.path\n",
    "import time\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "import NeuralNetwork.TensorFlow.input_data as input_data\n",
    "import NeuralNetwork.TensorFlow.mnist as mst\n",
    "# Arguments Definition\n",
    "\n",
    "learning_rate = 0.01\n",
    "max_steps = 2000\n",
    "hidden1 = 300\n",
    "hidden2 = 32\n",
    "batch_size = 100\n",
    "fake_data = False\n",
    "train_dir = 'data'\n",
    "\n",
    "# Load data_set\n",
    "data_sets = input_data.read_data_sets(train_dir, fake_data)\n",
    "\n",
    "\n",
    "\n",
    "                 \n",
    "def placeholder_inputs(batch_size):\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                         mst.IMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return images_placeholder, labels_placeholder\n",
    "\n",
    "def fill_feed_dict(data_set, batch_size, images_pl, labels_pl):\n",
    "  images_feed, labels_feed = data_set.next_batch(batch_size,fake_data)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict\n",
    "\n",
    "\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  \"\"\"Runs one evaluation against the full epoch of data.\n",
    "\n",
    "  Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "  \"\"\"\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // batch_size\n",
    "  num_examples = steps_per_epoch * batch_size\n",
    "  print(steps_per_epoch, num_examples)\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set, batch_size,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    count = sess.run(eval_correct, feed_dict=feed_dict)\n",
    "    true_count += count \n",
    "  precision = true_count / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n",
    "\n",
    "\n",
    "    \n",
    "# Tell TensorFlow that the model will be built into the default Graph.\n",
    "with tf.Graph().as_default() as g:\n",
    "    # Generate placeholders for the images and labels.\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(batch_size)\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits = mst.inference2(images_placeholder,hidden1)\n",
    "    #logits = mst.inference(images_placeholder,hidden1,hidden2)\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss = mst.loss(logits, labels_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op = mst.training(loss, learning_rate)\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct = mst.evaluation(logits, labels_placeholder)\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Start the training loop.\n",
    "    for step in xrange(max_steps):\n",
    "        start_time = time.time()\n",
    "        feed_dict = fill_feed_dict(data_sets.train, batch_size, images_placeholder, labels_placeholder)\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        duration = time.time() - start_time       \n",
    "        # Write the summaries and print an overview fairly often.\n",
    "        if step % 200 == 0:\n",
    "        # Print status to stdout.\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "\n",
    "        # Save a checkpoint and evaluate the model periodically.\n",
    "#         if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "#             print('Print logits:')\n",
    "#             print('Training Data Eval:')\n",
    "#             do_eval(sess,\n",
    "#                     eval_correct,\n",
    "#                     images_placeholder,\n",
    "#                     labels_placeholder,\n",
    "#                     data_sets.train)\n",
    "        \n",
    "#             # Evaluate against the validation set.\n",
    "#             print('Validation Data Eval:')\n",
    "#             do_eval(sess,\n",
    "#                     eval_correct,\n",
    "#                     images_placeholder,\n",
    "#                     labels_placeholder,\n",
    "#                     data_sets.validation)\n",
    "#             # Evaluate against the test set.\n",
    "#             print('Test Data Eval:')\n",
    "#             do_eval(sess,\n",
    "#                     eval_correct,\n",
    "#                     images_placeholder,\n",
    "#                     labels_placeholder,\n",
    "#                     data_sets.test)\n",
    "            \n",
    "    print('Final Eval:')\n",
    "    do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_sets.test)\n",
    "    print('Step %d: loss = %.2f' % (step, loss_value))    \n",
    "    \n",
    "    w1 = g.get_collection(tf.GraphKeys.VARIABLES, \"hidden1/weights\")\n",
    "    w2 = g.get_collection(tf.GraphKeys.VARIABLES, \"softmax_linear/weights\")\n",
    "    w1_val,w2_val = sess.run([w1,w2])\n",
    "#     print(w1_val,w2_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 300)\n",
      "(300, 10)\n"
     ]
    }
   ],
   "source": [
    "w1_v = sess.run(w1)\n",
    "print(w1_v[0].shape)\n",
    "\n",
    "w2_v = sess.run(w2)\n",
    "print(w2_v[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem:\n",
    "1. w is in size of 100: saver to record all of them then concat them together or adapt neuralnetwork.py\n",
    "2. how to check whether it gains better loss, if we don't have z, or let z = wa? is it not accurate because in our network it's not the case.\n",
    "3. mini-batch is much much much faster than normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 2000\n"
     ]
    }
   ],
   "source": [
    "log = sess.run(g.get_collection(tf.GraphKeys.VARIABLES, \"softmax_linear/weights\"))\n",
    "print(step,max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.08991498, -0.04007696,  0.03823632, ..., -0.04408926,\n",
       "         -0.06910585,  0.05061473],\n",
       "        [-0.11108518, -0.01257499,  0.11620861, ...,  0.08067908,\n",
       "          0.06203641, -0.09924033],\n",
       "        [-0.05920425, -0.04707693,  0.01908229, ..., -0.03129459,\n",
       "         -0.05405887, -0.04370198],\n",
       "        ..., \n",
       "        [-0.14072941,  0.0451174 , -0.0910515 , ...,  0.13482447,\n",
       "         -0.05188115,  0.13384832],\n",
       "        [ 0.0023597 ,  0.09977771,  0.01873406, ..., -0.05493696,\n",
       "         -0.09123113,  0.06701973],\n",
       "        [-0.041249  , -0.03007073, -0.0700364 , ..., -0.1045787 ,\n",
       "         -0.02126557, -0.05199299]], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000441887\n",
      "0.00327859\n",
      "0.00306615\n",
      "0.00108164\n",
      "0.00383746\n",
      "0.00617507\n",
      "0.00598686\n",
      "0.00462077\n",
      "0.00205038\n",
      "0.000815012\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "w = W.eval()\n",
    "[x,y]=np.where( w > 0.0 )\n",
    "for i in range(10):\n",
    "    print w[x[i],y[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n"
     ]
    }
   ],
   "source": [
    "print w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Layer Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8902\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initial input img and output label\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# Train network\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "sess.run(tf.initialize_all_variables())\n",
    "y = tf.matmul(x,W) #+ b\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Mini batch descent\n",
    "for i in range(100):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "# Predict\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
